# 03. 모델 아키텍처

## 🧠 3-모델 하이브리드 앙상블 설계

---

## 1. 아키텍처 개요

### 1.1 전체 구조

```
┌─────────────────────────────────────────────────────────────────┐
│                     입력 텍스트                                  │
│              "김치녀들은 진짜 답이 없다"                          │
└─────────────────────────────────────────────────────────────────┘
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│   KcELECTRA     │ │  SoongsilBERT   │ │  RoBERTa-Base   │
│                 │ │                 │ │                 │
│  슬랭/욕설 전문가 │ │  안정적 베이스라인 │ │ 고맥락 의미론 전문가│
│                 │ │                 │ │                 │
│  [확률 벡터]     │ │  [확률 벡터]     │ │  [확률 벡터]     │
│  (9차원)        │ │  (9차원)        │ │  (9차원)        │
└────────┬────────┘ └────────┬────────┘ └────────┬────────┘
         │                   │                   │
         └───────────────────┼───────────────────┘
                             ▼
              ┌─────────────────────────────┐
              │      가중 소프트 보팅         │
              │   (Weighted Soft Voting)    │
              │                             │
              │  P_final = Σ(w_i × P_i)     │
              └──────────────┬──────────────┘
                             ▼
              ┌─────────────────────────────┐
              │     클래스별 임계값 적용      │
              │                             │
              │  if P[i] > threshold[i]:    │
              │      label[i] = 1           │
              └──────────────┬──────────────┘
                             ▼
              ┌─────────────────────────────┐
              │         최종 예측            │
              │  [1, 0, 0, 0, 0, 0, 0, 0, 1] │
              │  (여성/가족 + 악플/욕설)      │
              └─────────────────────────────┘
```

### 1.2 설계 철학

**왜 3개 모델인가?**

| 단일 모델 한계 | 앙상블 해결책 |
|---------------|--------------|
| 특정 패턴에 과적합 | 다양한 관점 통합 |
| 일반화 성능 부족 | 상호 보완적 예측 |
| 불안정한 예측 | 안정적인 확률 평균 |

---

## 2. 개별 모델 상세

### 2.1 KcELECTRA

```
┌─────────────────────────────────────────────────────────────────┐
│                      KcELECTRA-Base                             │
├─────────────────────────────────────────────────────────────────┤
│  🏷️  역할: 슬랭/욕설 전문가                                      │
│  📦  출처: beomi/KcELECTRA-base                                 │
│  📊  파라미터: 약 1.1억 개                                       │
│  🎯  강점: 한국어 인터넷 언어에 특화                              │
├─────────────────────────────────────────────────────────────────┤
│  학습 데이터:                                                    │
│  • 한국어 뉴스, 위키피디아                                       │
│  • 온라인 댓글 데이터 (네이버 뉴스, 커뮤니티)                     │
│  • 신조어, 은어, 욕설 포함                                       │
├─────────────────────────────────────────────────────────────────┤
│  선택 이유:                                                      │
│  1. 인터넷 댓글 기반 사전학습 → 혐오 표현 이해도 높음            │
│  2. ELECTRA 아키텍처 → 효율적인 학습                            │
│  3. 한국어 신조어 토큰화 우수                                    │
└─────────────────────────────────────────────────────────────────┘
```

**HuggingFace**: `beomi/KcELECTRA-base`

### 2.2 SoongsilBERT

```
┌─────────────────────────────────────────────────────────────────┐
│                     SoongsilBERT-Base                           │
├─────────────────────────────────────────────────────────────────┤
│  🏷️  역할: 안정적 베이스라인                                     │
│  📦  출처: soongsil-ai/soongsil-bert-base                       │
│  📊  파라미터: 약 1.1억 개                                       │
│  🎯  강점: 균형 잡힌 범용 성능                                    │
├─────────────────────────────────────────────────────────────────┤
│  학습 데이터:                                                    │
│  • 한국어 위키피디아                                             │
│  • 한국어 뉴스 기사                                              │
│  • 한국어 책 말뭉치                                              │
├─────────────────────────────────────────────────────────────────┤
│  선택 이유:                                                      │
│  1. 다양한 도메인에서 균형 잡힌 성능                             │
│  2. 학술 기관에서 검증된 품질                                    │
│  3. 안정적인 기준점 역할                                         │
└─────────────────────────────────────────────────────────────────┘
```

**HuggingFace**: `soongsil-ai/soongsil-bert-base`

### 2.3 KLUE-RoBERTa-Base

```
┌─────────────────────────────────────────────────────────────────┐
│                    KLUE-RoBERTa-Base                            │
├─────────────────────────────────────────────────────────────────┤
│  🏷️  역할: 고맥락 의미론 전문가                                   │
│  📦  출처: klue/roberta-base                                    │
│  📊  파라미터: 약 1.1억 개                                       │
│  🎯  강점: 문맥 이해력 우수                                       │
├─────────────────────────────────────────────────────────────────┤
│  학습 데이터:                                                    │
│  • KLUE 벤치마크 코퍼스 (62GB)                                  │
│  • 모두의 말뭉치                                                 │
│  • 위키피디아, 뉴스, 청원 등                                     │
├─────────────────────────────────────────────────────────────────┤
│  선택 이유:                                                      │
│  1. 대규모 코퍼스로 문맥 이해력 우수                             │
│  2. 맥락 의존적 혐오 표현 탐지                                   │
│  3. RoBERTa 아키텍처의 안정성                                    │
└─────────────────────────────────────────────────────────────────┘
```

**HuggingFace**: `klue/roberta-base`

---

## 3. 분류기 헤드 설계

### 3.1 Multi-Label Classifier

```python
class MultiLabelClassifier(nn.Module):
    def __init__(self, model_name, num_labels=9, dropout_rate=0.3):
        super().__init__()
        
        # 1. 사전학습 모델 로드
        self.encoder = AutoModel.from_pretrained(model_name)
        hidden_size = self.encoder.config.hidden_size  # 768
        
        # 2. 분류기 헤드
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, num_labels)
        )
    
    def forward(self, input_ids, attention_mask):
        # [CLS] 토큰 임베딩 추출
        outputs = self.encoder(input_ids, attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [batch, 768]
        
        # 분류 (Sigmoid는 손실 함수에서 처리)
        logits = self.classifier(cls_embedding)  # [batch, 9]
        
        return logits
```

### 3.2 구조 다이어그램

```
입력 텍스트: "한남충들 진짜 싫다"
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                       토크나이저                                 │
│  [CLS] 한 ##남 ##충 ##들 진짜 싫 ##다 [SEP] [PAD] ...           │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Transformer Encoder                          │
│                  (12 layers, 768 hidden)                        │
│                                                                 │
│  [CLS] → ████ → ... → [Output: 768-dim vector]                 │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼ [CLS] 토큰 벡터 (768차원)
┌─────────────────────────────────────────────────────────────────┐
│                       Dropout (0.3)                             │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Linear (768 → 9)                             │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼ Logits (9차원)
┌─────────────────────────────────────────────────────────────────┐
│                       Sigmoid                                   │
│           (BCEWithLogitsLoss 내부에서 처리)                      │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼ 확률 (9차원, 각 0~1)
          [0.92, 0.03, 0.01, 0.02, 0.01, 0.01, 0.01, 0.02, 0.85]
          (여성/가족: 92%, 악플/욕설: 85%)
```

---

## 4. 앙상블 전략

### 4.1 가중 소프트 보팅 (Weighted Soft Voting)

```python
def ensemble_predict(models, texts, weights):
    """
    가중 소프트 보팅으로 앙상블 예측
    
    Args:
        models: [KcELECTRA, SoongsilBERT, RoBERTa]
        texts: 입력 텍스트 배치
        weights: [0.35, 0.33, 0.32]  # 모델별 가중치
    
    Returns:
        앙상블 확률 벡터
    """
    all_probs = []
    
    for model, weight in zip(models, weights):
        probs = model.predict_proba(texts)  # [batch, 9]
        all_probs.append(probs * weight)
    
    # 가중 평균
    ensemble_probs = sum(all_probs)  # [batch, 9]
    
    return ensemble_probs
```

### 4.2 모델 가중치 결정

| 모델 | 개별 F1 | 가중치 | 역할 |
|------|---------|--------|------|
| KcELECTRA | 80.2% | 0.35 | 슬랭 전문가 |
| SoongsilBERT | 79.5% | 0.33 | 베이스라인 |
| RoBERTa-Base | 78.8% | 0.32 | 맥락 전문가 |

**가중치 결정 방식**: 검증 데이터 F1-Macro 기반 비례 할당

### 4.3 앙상블 효과

```
┌─────────────────────────────────────────────────────────────────┐
│                     앙상블 효과 분석                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  개별 모델 F1-Macro:                                            │
│  ├── KcELECTRA:     ████████████████████░░░░  80.2%            │
│  ├── SoongsilBERT:  ███████████████████░░░░░  79.5%            │
│  └── RoBERTa-Base:  ██████████████████░░░░░░  78.8%            │
│                                                                 │
│  앙상블 F1-Macro:   ████████████████████████  82.91% (+2.7%p)  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 5. 클래스별 임계값 최적화

### 5.1 왜 임계값 최적화가 필요한가?

기본 임계값 0.5는 모든 클래스에 최적이 아닙니다.

```
문제:
├── 다수 클래스 (악플/욕설): 0.5에서 과탐지
├── 소수 클래스 (연령): 0.5에서 미탐지
└── 클래스별 분포가 다름
```

### 5.2 최적화 알고리즘

```python
def optimize_thresholds(val_probs, val_labels):
    """
    각 클래스별로 F1-Score를 최대화하는 임계값 탐색
    """
    optimal_thresholds = []
    
    for class_idx in range(9):
        best_threshold = 0.5
        best_f1 = 0
        
        # 0.01 ~ 0.99 범위에서 탐색
        for threshold in np.arange(0.01, 1.0, 0.01):
            preds = (val_probs[:, class_idx] > threshold).astype(int)
            f1 = f1_score(val_labels[:, class_idx], preds)
            
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        optimal_thresholds.append(best_threshold)
    
    return optimal_thresholds
```

### 5.3 최적화 결과

| 클래스 | 기본 임계값 | 최적 임계값 | F1 향상 |
|--------|------------|------------|---------|
| 여성/가족 | 0.50 | 0.50 | - |
| 남성 | 0.50 | 0.50 | - |
| 성소수자 | 0.50 | 0.53 | +1.2% |
| 인종/국적 | 0.50 | 0.29 | +3.5% |
| 연령 | 0.50 | 0.50 | - |
| 지역 | 0.50 | 0.50 | - |
| 종교 | 0.50 | 0.39 | +2.1% |
| 기타 혐오 | 0.50 | 0.50 | - |
| 악플/욕설 | 0.50 | 0.45 | +0.8% |

---

## 6. 추론 파이프라인

### 6.1 전체 추론 과정

```python
def predict(text):
    """단일 텍스트 추론"""
    
    # 1. 전처리
    text = clean_text(text)
    
    # 2. 각 모델별 토크나이징 및 예측
    probs_list = []
    for model, tokenizer, weight in zip(models, tokenizers, weights):
        inputs = tokenizer(text, return_tensors="pt", ...)
        with torch.no_grad():
            logits = model(**inputs)
            probs = torch.sigmoid(logits)
        probs_list.append(probs * weight)
    
    # 3. 가중 소프트 보팅
    ensemble_probs = sum(probs_list)  # [1, 9]
    
    # 4. 클래스별 임계값 적용
    predictions = []
    for i, (prob, threshold) in enumerate(zip(ensemble_probs[0], thresholds)):
        predictions.append(1 if prob > threshold else 0)
    
    return predictions
```

### 6.2 예측 예시

```
입력: "김치녀들은 진짜 답이 없다"

┌──────────────────────────────────────────────────────────────────┐
│ 모델별 확률:                                                     │
│                                                                  │
│ KcELECTRA:    [0.91, 0.05, 0.02, 0.03, 0.01, 0.02, 0.01, 0.02, 0.82]
│ SoongsilBERT: [0.88, 0.04, 0.01, 0.02, 0.01, 0.01, 0.01, 0.03, 0.79]
│ RoBERTa:      [0.85, 0.03, 0.02, 0.02, 0.01, 0.02, 0.02, 0.01, 0.75]
├──────────────────────────────────────────────────────────────────┤
│ 가중 평균:    [0.88, 0.04, 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.79]
├──────────────────────────────────────────────────────────────────┤
│ 임계값 적용:  [1,    0,    0,    0,    0,    0,    0,    0,    1   ]
├──────────────────────────────────────────────────────────────────┤
│ 최종 예측:    여성/가족 ✓, 악플/욕설 ✓                            │
└──────────────────────────────────────────────────────────────────┘
```

---

## 7. 코드 구조

### 7.1 src/model.py

```python
# 주요 클래스 및 함수

class MultiLabelClassifier(nn.Module):
    """다중 라벨 분류를 위한 기본 분류기"""
    ...

def create_model(model_name, num_labels, ...):
    """모델 팩토리 함수"""
    ...
```

### 7.2 모델 저장 형식

```
models/
├── kcelectra.pt      # KcELECTRA 가중치
├── soongsil.pt       # SoongsilBERT 가중치
└── roberta_base.pt   # RoBERTa-Base 가중치
```

**체크포인트 구조**:
```python
{
    'model_state_dict': model.state_dict(),
    'model_name': 'kcelectra',
    'best_f1': 0.802,
    'epoch': 45,
    'threshold': 0.5
}
```

**모델 다운로드**:
- 학습된 모델은 용량이 커서 GitHub에 포함되지 않습니다.
- [Google Drive](https://drive.google.com/drive/folders/1Noow6HkhI6hkAuggptroiNmbUVGDbu1u?usp=sharing)에서 다운로드 가능합니다.
- 다운로드 후 `models/` 폴더에 배치하세요.

---

**이전 문서**: [02_데이터_분석.md](02_데이터_분석.md)  
**다음 문서**: [04_학습_전략.md](04_학습_전략.md)

