# 04. í•™ìŠµ ì „ëµ

## ğŸ“ ëª¨ë¸ í•™ìŠµ ìƒì„¸ ê°€ì´ë“œ

---

## 1. í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê°œìš”

### 1.1 ì „ì²´ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      í•™ìŠµ íŒŒì´í”„ë¼ì¸                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„                                                 â”‚
â”‚     â”œâ”€â”€ TSV íŒŒì¼ ë¡œë“œ                                           â”‚
â”‚     â”œâ”€â”€ AEDA ë°ì´í„° ì¦ê°• (ì†Œìˆ˜ í´ë˜ìŠ¤)                           â”‚
â”‚     â””â”€â”€ Train/Val/Test ë¶„í•                                      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚  2ï¸âƒ£ ëª¨ë¸ë³„ í•™ìŠµ (3ê°œ ëª¨ë¸ ìˆœì°¨ í•™ìŠµ)                             â”‚
â”‚     â”œâ”€â”€ KcELECTRA (80 epochs)                                  â”‚
â”‚     â”œâ”€â”€ SoongsilBERT (80 epochs)                               â”‚
â”‚     â””â”€â”€ RoBERTa-Base (80 epochs)                               â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚  3ï¸âƒ£ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”                                        â”‚
â”‚     â””â”€â”€ ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²°ì •                           â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚  4ï¸âƒ£ í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™”                                      â”‚
â”‚     â””â”€â”€ F1-Macro ìµœëŒ€í™” ì„ê³„ê°’ íƒìƒ‰                              â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚  5ï¸âƒ£ ìµœì¢… í‰ê°€                                                   â”‚
â”‚     â””â”€â”€ Test ë°ì´í„°ë¡œ 1íšŒ í‰ê°€                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ì†Œìš” ì‹œê°„

| ë‹¨ê³„ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ |
|------|------|-----------|
| 1 | ë°ì´í„° ì¤€ë¹„ | 5ë¶„ |
| 2-1 | KcELECTRA í•™ìŠµ | 4-5ì‹œê°„ |
| 2-2 | SoongsilBERT í•™ìŠµ | 4-5ì‹œê°„ |
| 2-3 | RoBERTa-Base í•™ìŠµ | 4-5ì‹œê°„ |
| 3 | ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” | 30ë¶„ |
| 4 | ì„ê³„ê°’ ìµœì í™” | 30ë¶„ |
| 5 | ìµœì¢… í‰ê°€ | 10ë¶„ |
| **ì´ê³„** | | **ì•½ 15ì‹œê°„** |

---

## 2. ì†ì‹¤ í•¨ìˆ˜

### 2.1 BCEWithLogitsLoss

ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ Binary Cross-Entropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
```

**ì™œ BCEWithLogitsLossì¸ê°€?**

| íŠ¹ì„± | ì„¤ëª… |
|------|------|
| Multi-Label | ê° ë¼ë²¨ì„ ë…ë¦½ì ì¸ ì´ì§„ ë¶„ë¥˜ë¡œ ì²˜ë¦¬ |
| Numerical Stability | Sigmoid + BCEë¥¼ í•˜ë‚˜ë¡œ ê²°í•© (ì•ˆì •ì ) |
| pos_weight | í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ ê°€ëŠ¥ |

### 2.2 ìˆ˜ì‹

```
L = -[y * log(Ïƒ(x)) + (1-y) * log(1-Ïƒ(x))]

where:
- x: ëª¨ë¸ ì¶œë ¥ (logits)
- y: íƒ€ê²Ÿ ë ˆì´ë¸” (0 ë˜ëŠ” 1)
- Ïƒ: sigmoid í•¨ìˆ˜
```

### 2.3 í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (pos_weight)

ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

```python
def calculate_pos_weight(labels):
    """
    í´ë˜ìŠ¤ë³„ positive weight ê³„ì‚°
    
    pos_weight = num_negative / num_positive
    """
    pos_counts = labels.sum(axis=0)
    neg_counts = len(labels) - pos_counts
    
    pos_weight = neg_counts / pos_counts
    
    # ê³¼ë„í•œ ê°€ì¤‘ì¹˜ ë°©ì§€ (ìµœëŒ€ 10ë°°)
    pos_weight = torch.clamp(pos_weight, max=10.0)
    
    return pos_weight
```

| í´ë˜ìŠ¤ | Positive ë¹„ìœ¨ | pos_weight |
|--------|---------------|------------|
| ì•…í”Œ/ìš•ì„¤ | 20.9% | 3.8 |
| ì—°ë ¹ | 4.0% | 10.0 (clamped) |
| ê¸°íƒ€ í˜ì˜¤ | 3.7% | 10.0 (clamped) |

---

## 3. ì˜µí‹°ë§ˆì´ì €

### 3.1 AdamW

```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=2e-5,
    weight_decay=0.01,
    betas=(0.9, 0.999),
    eps=1e-8
)
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ëª…**:

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| `lr` | 2e-5 | í•™ìŠµë¥  (PLM fine-tuning í‘œì¤€) |
| `weight_decay` | 0.01 | L2 ì •ê·œí™” (ê³¼ì í•© ë°©ì§€) |
| `betas` | (0.9, 0.999) | ëª¨ë©˜í…€ ê³„ìˆ˜ |
| `eps` | 1e-8 | ìˆ˜ì¹˜ ì•ˆì •ì„± |

### 3.2 ì™œ AdamWì¸ê°€?

| íŠ¹ì„± | Adam | AdamW |
|------|------|-------|
| Weight Decay | L2 í˜ë„í‹° | Decoupled Weight Decay |
| ì¼ë°˜í™” ì„±ëŠ¥ | ë³´í†µ | ìš°ìˆ˜ |
| Transformer í˜¸í™˜ | ì¢‹ìŒ | ë§¤ìš° ì¢‹ìŒ |

---

## 4. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

### 4.1 Cosine Annealing with Warmup

```python
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)
```

### 4.2 ìŠ¤ì¼€ì¤„ ì‹œê°í™”

```
í•™ìŠµë¥ 
  ^
  â”‚    â•­â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚   â•±         â•²
  â”‚  â•±           â•²
  â”‚ â•±             â•²
  â”‚â•±               â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> ìŠ¤í…
     Warmup   Cosine Decay
```

### 4.3 ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê°’ | ê³„ì‚° |
|----------|-----|------|
| `warmup_steps` | 500 | ì „ì²´ì˜ ì•½ 5% |
| `total_steps` | ~10,000 | epochs Ã— steps_per_epoch |
| ì´ˆê¸° lr | 2e-5 | - |
| ìµœì¢… lr | ~0 | Cosine ê°ì‡  |

---

## 5. ì •ê·œí™” ê¸°ë²•

### 5.1 Dropout

```python
self.classifier = nn.Sequential(
    nn.Dropout(0.3),  # 30% dropout
    nn.Linear(hidden_size, num_labels)
)
```

**Dropout Rate: 0.3**

| Rate | íš¨ê³¼ |
|------|------|
| 0.1 | ì•½í•œ ì •ê·œí™” |
| **0.3** | ì ì ˆí•œ ì •ê·œí™” (ì„ íƒ) |
| 0.5 | ê°•í•œ ì •ê·œí™” (í•™ìŠµ ëŠë¦¼) |

### 5.2 Weight Decay

```python
optimizer = AdamW(..., weight_decay=0.01)
```

L2 ì •ê·œí™”ë¡œ ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ

### 5.3 Early Stopping

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
    
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Stop training
        else:
            self.best_score = score
            self.counter = 0
        return False
```

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| `patience` | 10 | ê°œì„  ì—†ì´ 10 epoch ëŒ€ê¸° |
| `min_delta` | 0.001 | ìµœì†Œ ê°œì„  í­ |

---

## 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½

### 6.1 í•™ìŠµ ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê°’ | ë¹„ê³  |
|----------|-----|------|
| **Epochs** | 80 | Early Stopping ì ìš© |
| **Batch Size** | 16 | GPU ë©”ëª¨ë¦¬ ê³ ë ¤ |
| **Max Length** | 128 | 95% ì»¤ë²„ë¦¬ì§€ |
| **Learning Rate** | 2e-5 | PLM í‘œì¤€ |
| **Weight Decay** | 0.01 | L2 ì •ê·œí™” |
| **Dropout** | 0.3 | ê³¼ì í•© ë°©ì§€ |
| **Warmup Steps** | 500 | ì „ì²´ì˜ 5% |
| **Early Stopping** | patience=10 | ê³¼ì í•© ë°©ì§€ |

### 6.2 ë°ì´í„° ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê°’ | ë¹„ê³  |
|----------|-----|------|
| **Train ë°ì´í„°** | ~15,000 | ì›ë³¸ + AEDA ì¦ê°• |
| **Val ë°ì´í„°** | ~1,500 | ì„ê³„ê°’ ìµœì í™”ìš© |
| **Test ë°ì´í„°** | ~1,500 | ìµœì¢… í‰ê°€ (1íšŒ) |
| **AEDA ì¦ê°•** | ì†Œìˆ˜ í´ë˜ìŠ¤ 2-3ë°° | ë¶ˆê· í˜• í•´ì†Œ |

---

## 7. í•™ìŠµ ë£¨í”„

### 7.1 Train ì½”ë“œ

```python
def train_epoch(model, train_loader, optimizer, scheduler, criterion, device):
    model.train()
    total_loss = 0
    
    for batch in tqdm(train_loader):
        # 1. ë°ì´í„° GPU ì´ë™
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        # 2. Forward
        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        
        # 3. Loss ê³„ì‚°
        loss = criterion(logits, labels)
        
        # 4. Backward
        loss.backward()
        
        # 5. Gradient Clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 6. Update
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
```

### 7.2 Validation ì½”ë“œ

```python
def validate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).int()
            
            total_loss += loss.item()
            all_preds.append(preds.cpu())
            all_labels.append(labels.cpu())
    
    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    f1_macro = f1_score(all_labels, all_preds, average='macro')
    
    return total_loss / len(val_loader), f1_macro
```

---

## 8. í•™ìŠµ ëª¨ë‹ˆí„°ë§

### 8.1 ë¡œê¹…

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì—í¬í¬ë³„ ë¡œê¹…
logger.info(f"Epoch {epoch+1}/{num_epochs}")
logger.info(f"  Train Loss: {train_loss:.4f}")
logger.info(f"  Val Loss: {val_loss:.4f}")
logger.info(f"  Val F1-Macro: {val_f1:.4f}")
```

### 8.2 ì²´í¬í¬ì¸íŠ¸ ì €ì¥

```python
def save_checkpoint(model, optimizer, epoch, best_f1, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_f1': best_f1
    }, path)
    logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
```

### 8.3 Best ëª¨ë¸ ì €ì¥ ì¡°ê±´

```python
if val_f1 > best_f1:
    best_f1 = val_f1
    save_checkpoint(model, optimizer, epoch, best_f1, f"models/{model_name}.pt")
    logger.info(f"  âœ“ Best ëª¨ë¸ ê°±ì‹ ! F1: {best_f1:.4f}")
```

---

## 9. GPU ë©”ëª¨ë¦¬ ìµœì í™”

### 9.1 Mixed Precision (FP16/BF16)

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Training loop
with autocast():
    logits = model(input_ids, attention_mask)
    loss = criterion(logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~50% ê°ì†Œ
- í•™ìŠµ ì†ë„ ~1.5ë°° í–¥ìƒ

### 9.2 Gradient Accumulation

```python
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps  # 16 * 4 = 64

for i, batch in enumerate(train_loader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 9.3 ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
# ë¶ˆí•„ìš”í•œ ìºì‹œ ì •ë¦¬
torch.cuda.empty_cache()

# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ì ˆì•½)
model.gradient_checkpointing_enable()
```

---

## 10. í•™ìŠµ ì‹¤í–‰

### 10.1 ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸
python train.py

# íŠ¹ì • ëª¨ë¸ë§Œ í•™ìŠµ
python train.py --model kcelectra

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½
python train.py --epochs 100 --lr 3e-5
```

### 10.2 í•™ìŠµ ë¡œê·¸ ì˜ˆì‹œ

```
2025-01-23 10:00:00 - INFO - ğŸš€ í•™ìŠµ ì‹œì‘: KcELECTRA
2025-01-23 10:00:05 - INFO - Epoch 1/80
2025-01-23 10:15:00 - INFO -   Train Loss: 0.4521
2025-01-23 10:15:30 - INFO -   Val Loss: 0.3892
2025-01-23 10:15:30 - INFO -   Val F1-Macro: 0.6234
2025-01-23 10:15:30 - INFO -   âœ“ Best ëª¨ë¸ ê°±ì‹ ! F1: 0.6234
...
2025-01-23 14:30:00 - INFO - Epoch 45/80
2025-01-23 14:45:00 - INFO -   Train Loss: 0.1234
2025-01-23 14:45:30 - INFO -   Val Loss: 0.1456
2025-01-23 14:45:30 - INFO -   Val F1-Macro: 0.8024
2025-01-23 14:45:30 - INFO -   âœ“ Best ëª¨ë¸ ê°±ì‹ ! F1: 0.8024
...
2025-01-23 15:00:00 - INFO - Early Stopping at Epoch 55
2025-01-23 15:00:00 - INFO - ğŸ‰ KcELECTRA í•™ìŠµ ì™„ë£Œ - Best F1: 0.8024
```

---

**ì´ì „ ë¬¸ì„œ**: [03_ëª¨ë¸_ì•„í‚¤í…ì²˜.md](03_ëª¨ë¸_ì•„í‚¤í…ì²˜.md)  
**ë‹¤ìŒ ë¬¸ì„œ**: [05_ì‹¤í—˜_ê²°ê³¼.md](05_ì‹¤í—˜_ê²°ê³¼.md)

