"""
í•™ìŠµëœ ì•™ìƒë¸” ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡
"""

import os
import torch
import numpy as np
import pandas as pd
import json
import logging
from tqdm import tqdm
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, accuracy_score, hamming_loss, classification_report

from data_loader import UnsmileDataLoader
from dataset import create_dataloaders
from model import create_model

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model_and_predict(model_config: dict, test_df: pd.DataFrame, label_columns: list, device: str):
    """ë‹¨ì¼ ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ (ê° ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©)"""

    model_name = model_config['name']
    model_path = f"./models/{model_name}.pt"

    if not os.path.exists(model_path):
        logger.warning(f"âš ï¸ {model_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    logger.info(f"ğŸ“¦ {model_name} ë¡œë”© ì¤‘...")

    # ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = AutoTokenizer.from_pretrained(model_config['hf_name'])

    # ëª¨ë¸ë³„ ë°ì´í„°ë¡œë” ìƒì„±
    _, _, test_loader = create_dataloaders(
        train_df=test_df[:1],  # dummy
        val_df=test_df[:1],    # dummy
        test_df=test_df,
        tokenizer=tokenizer,
        label_columns=label_columns,
        batch_size=16,
        max_length=128
    )

    # ëª¨ë¸ ìƒì„±
    model = create_model(
        model_name=model_config['hf_name'],
        num_labels=9,
        use_qlora=model_config.get('use_qlora', False)
    )

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(torch.load(model_path, map_location=device))
    model = model.to(device)
    model.eval()

    # ì˜ˆì¸¡
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f"Predicting {model_name}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            logits = outputs['logits']
            probs = torch.sigmoid(logits)

            all_probs.append(probs.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_probs = np.concatenate(all_probs, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    logger.info(f"âœ“ {model_name} ì˜ˆì¸¡ ì™„ë£Œ: {all_probs.shape}")

    return all_probs, all_labels


def ensemble_predict(models_config: list, test_df: pd.DataFrame, label_columns: list, device: str):
    """ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘")
    logger.info("="*80)

    all_model_probs = []
    labels = None

    for config in models_config:
        result = load_model_and_predict(config, test_df, label_columns, device)

        if result is not None:
            probs, lbls = result
            all_model_probs.append(probs)
            if labels is None:
                labels = lbls

    if len(all_model_probs) == 0:
        raise ValueError("ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")

    # ì•™ìƒë¸” (ë‹¨ìˆœ í‰ê· )
    ensemble_probs = np.mean(all_model_probs, axis=0)

    logger.info(f"\nâœ… ì•™ìƒë¸” ì™„ë£Œ: {len(all_model_probs)}ê°œ ëª¨ë¸ í‰ê· ")
    logger.info("="*80 + "\n")

    return ensemble_probs, labels


def apply_optimal_thresholds(probs: np.ndarray, thresholds: np.ndarray) -> np.ndarray:
    """ìµœì  ì„ê³„ê°’ ì ìš©í•˜ì—¬ ì˜ˆì¸¡"""
    preds = np.zeros_like(probs, dtype=int)

    for i in range(probs.shape[1]):
        preds[:, i] = (probs[:, i] >= thresholds[i]).astype(int)

    return preds


def evaluate_predictions(labels: np.ndarray, preds: np.ndarray, label_names: list):
    """ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š í‰ê°€ ê²°ê³¼")
    logger.info("="*80)

    # ì „ì²´ ë©”íŠ¸ë¦­
    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
    f1_micro = f1_score(labels, preds, average='micro', zero_division=0)
    exact_match = accuracy_score(labels, preds)
    hamming_acc = 1 - hamming_loss(labels, preds)

    logger.info(f"\nğŸ† ì „ì²´ ì„±ëŠ¥:")
    logger.info(f"  F1-Macro: {f1_macro:.4f} ({f1_macro*100:.2f}%)")
    logger.info(f"  F1-Micro: {f1_micro:.4f} ({f1_micro*100:.2f}%)")
    logger.info(f"  Exact Match: {exact_match:.4f} ({exact_match*100:.2f}%)")
    logger.info(f"  Hamming Accuracy: {hamming_acc:.4f} ({hamming_acc*100:.2f}%)")

    # í´ë˜ìŠ¤ë³„ F1
    logger.info(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ F1-Score:")
    class_f1_scores = f1_score(labels, preds, average=None, zero_division=0)

    for i, (name, score) in enumerate(zip(label_names, class_f1_scores)):
        logger.info(f"  {name:20s}: {score:.4f}")

    logger.info("="*80 + "\n")

    return {
        'f1_macro': float(f1_macro),
        'f1_micro': float(f1_micro),
        'exact_match': float(exact_match),
        'hamming_accuracy': float(hamming_acc),
        'class_f1_scores': {name: float(score) for name, score in zip(label_names, class_f1_scores)}
    }


def save_predictions(test_df: pd.DataFrame, preds: np.ndarray, probs: np.ndarray, label_columns: list):
    """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""

    # ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±
    results_df = test_df[['text']].copy()

    # ì˜ˆì¸¡ ë ˆì´ë¸” ì¶”ê°€
    for i, col in enumerate(label_columns):
        results_df[f'pred_{col}'] = preds[:, i]
        results_df[f'prob_{col}'] = probs[:, i]

    # ì‹¤ì œ ë ˆì´ë¸” ì¶”ê°€
    for i, col in enumerate(label_columns):
        results_df[f'true_{col}'] = test_df[col].values

    # ì €ì¥
    output_path = "./results/test_predictions.csv"
    os.makedirs("./results", exist_ok=True)
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')

    logger.info(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    """ë©”ì¸ ì¶”ë¡  íŒŒì´í”„ë¼ì¸"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ”® UnSmile í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ ")
    logger.info("="*80)

    # ë””ë°”ì´ìŠ¤
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"\nğŸ’» Device: {device}")

    # 1. ë°ì´í„° ë¡œë“œ
    logger.info("\nğŸ“Š Step 1: ë°ì´í„° ë¡œë”©")
    data_loader = UnsmileDataLoader(data_dir="./data")
    train_df, val_df, test_df = data_loader.load_processed_data()
    label_columns = data_loader.label_columns

    logger.info(f"âœ“ Test ìƒ˜í”Œ ìˆ˜: {len(test_df)}")

    # 2. í•™ìŠµ ê²°ê³¼ ë¡œë“œ
    logger.info("\nğŸ“Š Step 2: í•™ìŠµ ê²°ê³¼ ë¡œë”©")

    # continue_results.jsonì„ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
    results_path = "./results/continue_results.json"
    if not os.path.exists(results_path):
        results_path = "./results/final_results.json"

    if not os.path.exists(results_path):
        logger.error(f"âŒ í•™ìŠµ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.error("ë¨¼ì € train_continue.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        return

    with open(results_path, 'r', encoding='utf-8') as f:
        results = json.load(f)

    logger.info(f"âœ“ ê²°ê³¼ íŒŒì¼ ë¡œë“œ: {results_path}")
    optimal_thresholds = np.array(results['optimal_thresholds'])
    logger.info(f"âœ“ ìµœì  ì„ê³„ê°’: {optimal_thresholds}")

    # 3. ëª¨ë¸ ì„¤ì • (3ê°œ ëª¨ë¸ ì•™ìƒë¸”)
    models_config = [
        {
            'name': 'kcelectra',
            'hf_name': 'beomi/KcELECTRA-base',
            'use_qlora': False
        },
        {
            'name': 'soongsil',
            'hf_name': 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',
            'use_qlora': False
        },
        {
            'name': 'roberta_base',
            'hf_name': 'klue/roberta-base',
            'use_qlora': False
        }
    ]

    logger.info(f"âœ“ ì•™ìƒë¸” ëª¨ë¸: {len(models_config)}ê°œ")
    for config in models_config:
        logger.info(f"  - {config['name']}: {config['hf_name']}")

    # 4. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± (ì²« ë²ˆì§¸ ëª¨ë¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
    logger.info("\nğŸ“Š Step 3: ë°ì´í„°ë¡œë” ìƒì„±")
    tokenizer = AutoTokenizer.from_pretrained(models_config[0]['hf_name'])

    _, _, test_loader = create_dataloaders(
        train_df=train_df,
        val_df=val_df,
        test_df=test_df,
        tokenizer=tokenizer,
        label_columns=label_columns,
        batch_size=16,
        max_length=128
    )

    # 5. ì•™ìƒë¸” ì˜ˆì¸¡
    logger.info("\nğŸ“Š Step 4: ì•™ìƒë¸” ì˜ˆì¸¡")
    ensemble_probs, test_labels = ensemble_predict(models_config, test_df, label_columns, device)

    # 6. ìµœì  ì„ê³„ê°’ ì ìš©
    logger.info("\nğŸ“Š Step 5: ìµœì  ì„ê³„ê°’ ì ìš©")
    ensemble_preds = apply_optimal_thresholds(ensemble_probs, optimal_thresholds)

    logger.info(f"âœ“ ì˜ˆì¸¡ ì™„ë£Œ: {ensemble_preds.shape}")
    logger.info(f"âœ“ ê¸ì • ì˜ˆì¸¡ ë¹„ìœ¨: {ensemble_preds.mean()*100:.2f}%")

    # 7. í‰ê°€
    logger.info("\nğŸ“Š Step 6: ìµœì¢… í‰ê°€")
    final_metrics = evaluate_predictions(test_labels, ensemble_preds, label_columns)

    # 8. ê²°ê³¼ ì €ì¥
    logger.info("\nğŸ“Š Step 7: ê²°ê³¼ ì €ì¥")
    save_predictions(test_df, ensemble_preds, ensemble_probs, label_columns)

    # ìµœì¢… ë©”íŠ¸ë¦­ ì €ì¥
    final_metrics['optimal_thresholds'] = optimal_thresholds.tolist()

    with open("./results/test_results.json", 'w', encoding='utf-8') as f:
        json.dump(final_metrics, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: ./results/test_results.json")

    # 9. ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
    logger.info("="*80)

    target_accuracy = 0.95
    achieved = final_metrics['hamming_accuracy'] >= target_accuracy

    if achieved:
        logger.info(f"âœ… ëª©í‘œ ë‹¬ì„±! Hamming Accuracy {final_metrics['hamming_accuracy']:.4f} >= {target_accuracy}")
    else:
        gap = target_accuracy - final_metrics['hamming_accuracy']
        logger.info(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±. ë¶€ì¡±: {gap:.4f} ({gap*100:.2f}%p)")
        logger.info(f"   í˜„ì¬: {final_metrics['hamming_accuracy']:.4f}")
        logger.info(f"   ëª©í‘œ: {target_accuracy}")

    logger.info("="*80 + "\n")


if __name__ == "__main__":
    main()
