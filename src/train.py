"""
15ì‹œê°„ ìµœëŒ€ ì •í™•ë„ ì „ëµ (15ë²ˆ ë¬¸ì„œ ê¸°ë°˜)
- 3ê°œ ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” (ê° ì „ë¬¸ ë¶„ì•¼)
- ëª¨ë¸ë‹¹ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ (80 epoch)
- í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™”
- ê°€ì¤‘ ì†Œí”„íŠ¸ ë³´íŒ…
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, get_cosine_schedule_with_warmup
from sklearn.metrics import f1_score, accuracy_score, hamming_loss
import numpy as np
import pandas as pd
import logging
from tqdm import tqdm
import json
import time
from scipy.optimize import minimize

# ë¡œì»¬ ëª¨ë“ˆ
from data_loader import UnsmileDataLoader
from aeda_augmentation import augment_minority_classes
from asymmetric_loss import AsymmetricLossOptimized
from dataset import create_dataloaders
from model import create_model

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Trainer:
    """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler,
        device: str,
        model_name: str
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.model_name = model_name

        self.best_f1 = 0.0
        self.best_model_state = None

    def train_epoch(self) -> float:
        """1 ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0

        progress_bar = tqdm(self.train_loader, desc=f"Training {self.model_name}")

        for batch in progress_bar:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Forward
            outputs = self.model(input_ids, attention_mask)
            logits = outputs['logits']

            # Loss ê³„ì‚°
            loss = self.criterion(logits, labels)

            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(self.train_loader)
        return avg_loss

    def evaluate(self, threshold: float = 0.5) -> dict:
        """ê²€ì¦ ë°ì´í„° í‰ê°€"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc=f"Evaluating {self.model_name}"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask)
                logits = outputs['logits']
                probs = torch.sigmoid(logits)

                all_probs.append(probs.cpu())
                all_labels.append(labels.cpu())

        # ê²°ê³¼ ì§‘ê³„
        all_probs = torch.cat(all_probs).numpy()
        all_labels = torch.cat(all_labels).numpy()
        all_preds = (all_probs >= threshold).astype(int)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
        f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)
        exact_match = accuracy_score(all_labels, all_preds)
        hamming_acc = 1 - hamming_loss(all_labels, all_preds)

        return {
            'f1_macro': f1_macro,
            'f1_micro': f1_micro,
            'exact_match': exact_match,
            'hamming_accuracy': hamming_acc,
            'probs': all_probs,
            'labels': all_labels
        }

    def train(self, num_epochs: int, patience: int = 10):
        """ì „ì²´ í•™ìŠµ ë£¨í”„"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ {self.model_name} í•™ìŠµ ì‹œì‘")
        logger.info(f"{'='*80}")

        no_improve = 0

        for epoch in range(num_epochs):
            logger.info(f"\nğŸ“Œ Epoch {epoch+1}/{num_epochs}")

            # í•™ìŠµ
            train_loss = self.train_epoch()
            logger.info(f"  Train Loss: {train_loss:.4f}")

            # í‰ê°€
            metrics = self.evaluate()
            logger.info(f"  Val F1-Macro: {metrics['f1_macro']:.4f}")
            logger.info(f"  Val Exact Match: {metrics['exact_match']:.4f}")
            logger.info(f"  Val Hamming Acc: {metrics['hamming_accuracy']:.4f}")

            # ìµœì  ëª¨ë¸ ì €ì¥
            if metrics['f1_macro'] > self.best_f1:
                self.best_f1 = metrics['f1_macro']
                self.best_model_state = self.model.state_dict()
                no_improve = 0
                logger.info(f"  âœ… ìƒˆë¡œìš´ ìµœê³  F1: {self.best_f1:.4f}")
            else:
                no_improve += 1
                logger.info(f"  â³ No improvement: {no_improve}/{patience}")

            # Early Stopping
            if no_improve >= patience:
                logger.info(f"\nâš ï¸ Early stopping at epoch {epoch+1}")
                break

        # ìµœì  ëª¨ë¸ ë³µì›
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            logger.info(f"\nâœ… ìµœì  ëª¨ë¸ ë³µì› ì™„ë£Œ (F1: {self.best_f1:.4f})")

        return self.best_f1


def train_single_model(
    model_config: dict,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: str,
    num_epochs: int = 80
):
    """ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ"""

    model_name = model_config['name']
    hf_model_name = model_config['hf_name']
    use_qlora = model_config.get('use_qlora', False)

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“¦ ëª¨ë¸ ì´ˆê¸°í™”: {model_name}")
    logger.info(f"{'='*80}")

    # ëª¨ë¸ ìƒì„±
    model = create_model(
        model_name=hf_model_name,
        num_labels=9,
        use_qlora=use_qlora
    )

    # ì†ì‹¤ í•¨ìˆ˜ - ì¼ë°˜ BCE Loss with Pos Weight (ë¶ˆê· í˜• ì²˜ë¦¬)
    # í´ë˜ìŠ¤ë³„ ì–‘ì„± ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    pos_weight = torch.ones(9) * 2.0  # ê¸ì • ìƒ˜í”Œì— 2ë°° ê°€ì¤‘ì¹˜
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))

    logger.info(f"  ì†ì‹¤ í•¨ìˆ˜: BCEWithLogitsLoss (pos_weight=2.0)")

    # ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=2e-5,
        weight_decay=0.01
    )

    # ìŠ¤ì¼€ì¤„ëŸ¬ (Cosine Annealing with Warmup)
    num_training_steps = len(train_loader) * num_epochs
    num_warmup_steps = int(0.1 * num_training_steps)

    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )

    # Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        model_name=model_name
    )

    # í•™ìŠµ
    best_f1 = trainer.train(num_epochs=num_epochs, patience=10)

    # ìµœì¢… í‰ê°€ (í™•ë¥ ê°’ ë°˜í™˜)
    final_metrics = trainer.evaluate()

    # ëª¨ë¸ ì €ì¥
    save_path = f"./models/{model_name}.pt"
    os.makedirs("./models", exist_ok=True)
    torch.save(trainer.model.state_dict(), save_path)
    logger.info(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")

    return trainer.model, final_metrics


def optimize_ensemble_weights(all_probs, true_labels, num_models):
    """
    ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” (Nelder-Mead)
    
    Args:
        all_probs: ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  ë¦¬ìŠ¤íŠ¸ [(n_samples, n_classes), ...]
        true_labels: ì‹¤ì œ ë ˆì´ë¸” (n_samples, n_classes)
        num_models: ëª¨ë¸ ê°œìˆ˜
    
    Returns:
        ìµœì  ê°€ì¤‘ì¹˜ ë°°ì—´
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” (Nelder-Mead)")
    logger.info("="*80)
    
    def objective(weights):
        """F1-Scoreë¥¼ ìµœëŒ€í™” (ìŒìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìµœì†Œí™” ë¬¸ì œë¡œ)"""
        weights = np.abs(weights)
        weights = weights / np.sum(weights)  # ì •ê·œí™”
        
        # ê°€ì¤‘ í‰ê· 
        ensemble_probs = sum(w * probs for w, probs in zip(weights, all_probs))
        
        # 0.5 ì„ê³„ê°’ìœ¼ë¡œ ì˜ˆì¸¡
        preds = (ensemble_probs >= 0.5).astype(int)
        
        # F1-Macro ê³„ì‚°
        f1 = f1_score(true_labels, preds, average='macro', zero_division=0)
        
        return -f1  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜
    
    # ì´ˆê¸° ê°€ì¤‘ì¹˜ (ê· ë“±)
    initial_weights = np.ones(num_models) / num_models
    
    # ìµœì í™”
    result = minimize(
        objective,
        initial_weights,
        method='Nelder-Mead',
        options={'maxiter': 500, 'disp': True}
    )
    
    optimal_weights = np.abs(result.x)
    optimal_weights = optimal_weights / np.sum(optimal_weights)
    
    logger.info(f"\nâœ… ìµœì  ê°€ì¤‘ì¹˜: {optimal_weights}")
    logger.info(f"  ìµœì  F1-Score: {-result.fun:.4f}")
    logger.info("="*80 + "\n")
    
    return optimal_weights


def optimize_thresholds_per_class(probs: np.ndarray, labels: np.ndarray, num_classes: int) -> np.ndarray:
    """
    í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰ (F1-Score ìµœëŒ€í™”)
    15ë²ˆ ë¬¸ì„œ ì „ëµ: ê° í´ë˜ìŠ¤ë§ˆë‹¤ 0.01~0.99 ë²”ìœ„ì—ì„œ ìµœì ê°’ íƒìƒ‰
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™” (ì •ë°€ íƒìƒ‰)")
    logger.info("="*80)

    optimal_thresholds = []

    for class_idx in range(num_classes):
        best_f1 = 0.0
        best_threshold = 0.5

        # 0.01~0.99 ë²”ìœ„ì—ì„œ íƒìƒ‰ (ë¬¸ì„œ ì „ëµ)
        for threshold in np.arange(0.01, 1.0, 0.01):
            preds = (probs[:, class_idx] >= threshold).astype(int)
            f1 = f1_score(labels[:, class_idx], preds, zero_division=0)

            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

        optimal_thresholds.append(best_threshold)
        logger.info(f"  Class {class_idx}: threshold={best_threshold:.2f}, F1={best_f1:.4f}")

    optimal_thresholds = np.array(optimal_thresholds)
    logger.info(f"\nâœ… ìµœì  ì„ê³„ê°’ íƒìƒ‰ ì™„ë£Œ")
    logger.info("="*80 + "\n")

    return optimal_thresholds


def emergency_boost_strategy(
    all_models,
    train_df_augmented,
    val_df,
    label_columns,
    device,
    current_f1,
    current_hamming
):
    """
    ê¸´ê¸‰ ì„±ëŠ¥ í–¥ìƒ ì „ëµ
    90% ë¯¸ë‹¬ì„± ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì¶”ê°€ ìµœì í™”
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸš¨ ê¸´ê¸‰ ì„±ëŠ¥ í–¥ìƒ ì „ëµ ì‹¤í–‰!")
    logger.info("="*80)
    logger.info(f"í˜„ì¬ ì„±ëŠ¥: F1-Macro={current_f1:.4f}, Hamming Acc={current_hamming:.4f}")
    logger.info("ëª©í‘œ: 90% ì´ìƒ ë‹¬ì„± í•„ìˆ˜!")

    # ì „ëµ 1: ë” ë§ì€ ë°ì´í„° ì¦ê°•
    logger.info("\nğŸ”„ ì „ëµ 1: ê°•í™”ëœ ë°ì´í„° ì¦ê°•")
    from src.data_loader import UnsmileDataLoader
    from src.aeda_augmentation import augment_minority_classes

    data_loader = UnsmileDataLoader(data_dir="./data")
    train_df, _, _ = data_loader.load_processed_data()

    # ì¦ê°• ê°•ë„ 2ë°° ì¦ê°€
    train_df_boosted = augment_minority_classes(
        train_df=train_df,
        label_columns=label_columns,
        target_size=2500,  # 1500 â†’ 2500
        punc_ratio=0.4,    # 0.3 â†’ 0.4
        augment_all=True   # ëª¨ë“  í´ë˜ìŠ¤ ì¦ê°•
    )

    logger.info(f"  ì¦ê°•ëœ ë°ì´í„°: {len(train_df)} â†’ {len(train_df_boosted)}")

    # ì „ëµ 2: ê° ëª¨ë¸ ì¶”ê°€ í•™ìŠµ (30 epoch)
    logger.info("\nğŸ”„ ì „ëµ 2: ëª¨ë¸ ì¶”ê°€ í•™ìŠµ (30 epoch)")

    boosted_probs = []

    for model_info in all_models:
        model_name = model_info['name']
        model = model_info['model']
        tokenizer = model_info['tokenizer']

        logger.info(f"\n  ğŸ“¦ {model_name} ì¶”ê°€ í•™ìŠµ ì¤‘...")

        from src.dataset import create_dataloaders

        train_loader, val_loader, _ = create_dataloaders(
            train_df=train_df_boosted,
            val_df=val_df,
            test_df=val_df,
            tokenizer=tokenizer,
            label_columns=label_columns,
            batch_size=32,
            max_length=128
        )

        # ì¶”ê°€ í•™ìŠµ (ë‚®ì€ learning rate)
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=1e-5,  # ë” ë‚®ì€ learning rate
            weight_decay=0.01
        )

        criterion = AsymmetricLossOptimized(
            gamma_neg=4.0,
            gamma_pos=0.0,
            clip=0.05
        )

        num_training_steps = len(train_loader) * 30
        num_warmup_steps = int(0.05 * num_training_steps)

        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )

        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
            model_name=f"{model_name}_boost"
        )

        trainer.train(num_epochs=30, patience=5)

        # í‰ê°€
        metrics = trainer.evaluate()
        boosted_probs.append(metrics['probs'])

        logger.info(f"  âœ… {model_name} ì¶”ê°€ í•™ìŠµ ì™„ë£Œ - F1: {metrics['f1_macro']:.4f}")

    # ì „ëµ 3: ì¬ìµœì í™”
    logger.info("\nğŸ”„ ì „ëµ 3: ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë° ì„ê³„ê°’ ì¬ìµœì í™”")

    y_val = val_df[label_columns].values

    # ê°€ì¤‘ì¹˜ ì¬ìµœì í™”
    optimal_weights = optimize_ensemble_weights(
        all_probs=boosted_probs,
        true_labels=y_val,
        num_models=len(boosted_probs)
    )

    # ì•™ìƒë¸”
    ensemble_probs = sum(w * probs for w, probs in zip(optimal_weights, boosted_probs))

    # ì„ê³„ê°’ ì¬ìµœì í™”
    optimal_thresholds = optimize_thresholds_per_class(
        probs=ensemble_probs,
        labels=y_val,
        num_classes=len(label_columns)
    )

    # ìµœì¢… í‰ê°€
    final_preds = (ensemble_probs >= optimal_thresholds).astype(int)

    boosted_f1 = f1_score(y_val, final_preds, average='macro', zero_division=0)
    boosted_hamming = 1 - hamming_loss(y_val, final_preds)

    logger.info(f"\nâœ… ê¸´ê¸‰ ì „ëµ ê²°ê³¼:")
    logger.info(f"  F1-Macro: {current_f1:.4f} â†’ {boosted_f1:.4f} (+{boosted_f1-current_f1:.4f})")
    logger.info(f"  Hamming Acc: {current_hamming:.4f} â†’ {boosted_hamming:.4f} (+{boosted_hamming-current_hamming:.4f})")

    return {
        'probs': boosted_probs,
        'weights': optimal_weights,
        'thresholds': optimal_thresholds,
        'f1_macro': boosted_f1,
        'hamming_acc': boosted_hamming
    }


def main():
    """15ì‹œê°„ ìµœëŒ€ ì •í™•ë„ ì „ëµ ë©”ì¸ í•¨ìˆ˜"""

    start_time = time.time()

    logger.info("\n" + "="*80)
    logger.info("ğŸš€ 15ì‹œê°„ ìµœëŒ€ ì •í™•ë„ ì „ëµ ì‹œì‘ (15ë²ˆ ë¬¸ì„œ)")
    logger.info("="*80)
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("\nğŸ“‹ ì „ëµ ê°œìš”:")
    logger.info("  1ï¸âƒ£ 3-ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” (ì—­í•  ë¶„ë‹´)")
    logger.info("  2ï¸âƒ£ ëª¨ë¸ë‹¹ 80 epoch (ì¶©ë¶„í•œ í•™ìŠµ)")
    logger.info("  3ï¸âƒ£ AEDA ì¦ê°• (ì†Œìˆ˜ í´ë˜ìŠ¤ ê°•í™”)")
    logger.info("  4ï¸âƒ£ Asymmetric Loss (ë¶ˆê· í˜• í•´ê²°)")
    logger.info("  5ï¸âƒ£ í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™”")
    logger.info("  6ï¸âƒ£ ê°€ì¤‘ ì†Œí”„íŠ¸ ë³´íŒ…")
    logger.info("  ğŸš¨ 7ï¸âƒ£ 90% ë¯¸ë‹¬ì„± ì‹œ ìë™ ê¸´ê¸‰ ì „ëµ ì‹¤í–‰!")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"\nğŸ’» Device: {device}")
    
    if device == 'cuda':
        logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")

    # 1. ë°ì´í„° ë¡œë“œ
    data_loader = UnsmileDataLoader(data_dir="./data")
    train_df, val_df, test_df = data_loader.load_processed_data()
    label_columns = data_loader.label_columns

    logger.info(f"\nğŸ“Š ë°ì´í„° í¬ê¸°:")
    logger.info(f"  Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

    # 2. AEDA ë°ì´í„° ì¦ê°• (15ë²ˆ ë¬¸ì„œ ì „ëµ)
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š Phase 1: AEDA ë°ì´í„° ì¦ê°• (ì†Œìˆ˜ í´ë˜ìŠ¤ ê°•í™”)")
    logger.info("="*80)

    train_df_augmented = augment_minority_classes(
        train_df=train_df,
        label_columns=label_columns,
        target_size=1500,  # ë¬¸ì„œ ê¶Œì¥ê°’
        punc_ratio=0.3,
        augment_all=False  # ì†Œìˆ˜ í´ë˜ìŠ¤ë§Œ
    )

    # 3. 3-ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” (15ë²ˆ ë¬¸ì„œ ì „ëµ)
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š Phase 2: 3-ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” í•™ìŠµ")
    logger.info("="*80)
    logger.info("\nê° ëª¨ë¸ì˜ ì—­í• :")
    logger.info("  ğŸ”¹ KcELECTRA-Base: ìŠ¬ë­/ìš•ì„¤ ì „ë¬¸ê°€ (ëŒ“ê¸€ ë°ì´í„° í•™ìŠµ)")
    logger.info("  ğŸ”¹ SoongsilBERT-Base: ì•ˆì •ì  ë² ì´ìŠ¤ë¼ì¸ (ê· í˜•ì¡íŒ ì„±ëŠ¥)")
    logger.info("  ğŸ”¹ KLUE-RoBERTa-Large+LoRA: ê³ ë§¥ë½ ì˜ë¯¸ë¡  ì „ë¬¸ê°€ (ë³µì¡í•œ í˜ì˜¤)")

    # 15ë²ˆ ë¬¸ì„œ ê¶Œì¥ 3ê°œ ëª¨ë¸
    models_config = [
        {
            'name': 'kcelectra',
            'hf_name': 'beomi/KcELECTRA-base',
            'use_qlora': False,
            'role': 'ìŠ¬ë­/ìš•ì„¤ ì „ë¬¸ê°€'
        },
        {
            'name': 'soongsil',
            'hf_name': 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',
            'use_qlora': False,
            'role': 'ì•ˆì •ì  ë² ì´ìŠ¤ë¼ì¸'
        },
        {
            'name': 'roberta_large',
            'hf_name': 'klue/roberta-large',
            'use_qlora': True,  # QLoRA í™œì„±í™”
            'role': 'ê³ ë§¥ë½ ì˜ë¯¸ë¡  ì „ë¬¸ê°€'
        }
    ]

    all_models = []
    all_val_probs = []

    for idx, config in enumerate(models_config, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ”¹ ëª¨ë¸ {idx}/3: {config['name']} ({config['role']})")
        logger.info(f"{'='*80}")

        tokenizer = AutoTokenizer.from_pretrained(config['hf_name'])

        train_loader, val_loader, test_loader = create_dataloaders(
            train_df=train_df_augmented,
            val_df=val_df,
            test_df=test_df,
            tokenizer=tokenizer,
            label_columns=label_columns,
            batch_size=32,
            max_length=128
        )

        try:
            model, metrics = train_single_model(
                model_config=config,
                train_loader=train_loader,
                val_loader=val_loader,
                device=device,
                num_epochs=80  # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„
            )

            all_models.append({
                'name': config['name'],
                'model': model,
                'tokenizer': tokenizer,
                'role': config['role']
            })
            all_val_probs.append(metrics['probs'])

            logger.info(f"\nâœ… {config['name']} í•™ìŠµ ì™„ë£Œ")
            logger.info(f"  F1-Macro: {metrics['f1_macro']:.4f}")

        except Exception as e:
            logger.error(f"âŒ {config['name']} ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue

    if len(all_models) == 0:
        logger.error("âŒ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨!")
        return None

    # 4. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” (Nelder-Mead)
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š Phase 3: ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”")
    logger.info("="*80)

    y_val = val_df[label_columns].values

    optimal_weights = optimize_ensemble_weights(
        all_probs=all_val_probs,
        true_labels=y_val,
        num_models=len(all_models)
    )

    # ê°€ì¤‘ ì•™ìƒë¸” í™•ë¥  ê³„ì‚°
    ensemble_probs = sum(w * probs for w, probs in zip(optimal_weights, all_val_probs))

    # 5. í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™” (15ë²ˆ ë¬¸ì„œ í•µì‹¬ ì „ëµ)
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š Phase 4: í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™”")
    logger.info("="*80)

    optimal_thresholds = optimize_thresholds_per_class(
        probs=ensemble_probs,
        labels=y_val,
        num_classes=len(label_columns)
    )

    # ìµœì¢… ì˜ˆì¸¡
    final_preds = (ensemble_probs >= optimal_thresholds).astype(int)

    # í‰ê°€
    final_f1_macro = f1_score(y_val, final_preds, average='macro', zero_division=0)
    final_f1_micro = f1_score(y_val, final_preds, average='micro', zero_division=0)
    final_exact_match = accuracy_score(y_val, final_preds)
    final_hamming_acc = 1 - hamming_loss(y_val, final_preds)

    # ğŸš¨ ê¸´ê¸‰ ì „ëµ: 90% ë¯¸ë‹¬ì„± ì‹œ ìë™ ì‹¤í–‰
    TARGET_THRESHOLD = 0.90

    if final_hamming_acc < TARGET_THRESHOLD or final_f1_macro < TARGET_THRESHOLD:
        logger.warning(f"\nâš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±! (Hamming: {final_hamming_acc:.4f}, F1: {final_f1_macro:.4f})")
        logger.info("ğŸš¨ ê¸´ê¸‰ ì„±ëŠ¥ í–¥ìƒ ì „ëµ ìë™ ì‹¤í–‰...")

        boost_result = emergency_boost_strategy(
            all_models=all_models,
            train_df_augmented=train_df_augmented,
            val_df=val_df,
            label_columns=label_columns,
            device=device,
            current_f1=final_f1_macro,
            current_hamming=final_hamming_acc
        )

        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        all_val_probs = boost_result['probs']
        optimal_weights = boost_result['weights']
        optimal_thresholds = boost_result['thresholds']
        final_f1_macro = boost_result['f1_macro']
        final_hamming_acc = boost_result['hamming_acc']

        # ì¬ê³„ì‚°
        ensemble_probs = sum(w * probs for w, probs in zip(optimal_weights, all_val_probs))
        final_preds = (ensemble_probs >= optimal_thresholds).astype(int)
        final_f1_micro = f1_score(y_val, final_preds, average='micro', zero_division=0)
        final_exact_match = accuracy_score(y_val, final_preds)

    # ê²°ê³¼ ì¶œë ¥
    elapsed_time = (time.time() - start_time) / 3600  # ì‹œê°„ ë‹¨ìœ„

    logger.info("\n" + "="*80)
    logger.info("ğŸ† ìµœì¢… ê²°ê³¼ (3-ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”)")
    logger.info("="*80)
    logger.info(f"\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
    logger.info(f"  F1-Macro:        {final_f1_macro:.4f} ({final_f1_macro*100:.2f}%)")
    logger.info(f"  F1-Micro:        {final_f1_micro:.4f} ({final_f1_micro*100:.2f}%)")
    logger.info(f"  Exact Match:     {final_exact_match:.4f} ({final_exact_match*100:.2f}%)")
    logger.info(f"  Hamming Acc:     {final_hamming_acc:.4f} ({final_hamming_acc*100:.2f}%)")
    logger.info(f"\nâ±ï¸  ì†Œìš” ì‹œê°„:        {elapsed_time:.2f} ì‹œê°„")
    logger.info(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")

    if final_hamming_acc >= TARGET_THRESHOLD and final_f1_macro >= TARGET_THRESHOLD:
        logger.info(f"  âœ…âœ…âœ… 90% ëª©í‘œ ë‹¬ì„± ì„±ê³µ! âœ…âœ…âœ…")
    else:
        logger.warning(f"  âš ï¸ 90% ëª©í‘œ ë¯¸ë‹¬ì„±...")
        logger.warning(f"  í˜„ì¬ ìµœê³  ì„±ëŠ¥: Hamming {final_hamming_acc*100:.2f}%, F1 {final_f1_macro*100:.2f}%")

    logger.info(f"  ì •í™•ë„ 95%:      {'âœ… ë‹¬ì„±!' if final_hamming_acc >= 0.95 else 'âŒ ë¯¸ë‹¬ì„±'}")
    logger.info(f"  F1-Macro 92%:    {'âœ… ë‹¬ì„±!' if final_f1_macro >= 0.92 else 'âŒ ë¯¸ë‹¬ì„±'}")
    logger.info("\nğŸ“‹ ì‚¬ìš©ëœ ëª¨ë¸:")
    for i, model_info in enumerate(all_models, 1):
        logger.info(f"  {i}. {model_info['name']:30s} (ê°€ì¤‘ì¹˜: {optimal_weights[i-1]:.3f}) - {model_info['role']}")
    logger.info("="*80)

    # ê²°ê³¼ ì €ì¥
    results = {
        'strategy': '15hour-3model-hybrid-ensemble',
        'document_reference': '15ë²ˆ ë¬¸ì„œ ì „ëµ',
        'n_models': len(all_models),
        'final_metrics': {
            'f1_macro': float(final_f1_macro),
            'f1_micro': float(final_f1_micro),
            'exact_match': float(final_exact_match),
            'hamming_accuracy': float(final_hamming_acc)
        },
        'optimal_weights': optimal_weights.tolist(),
        'optimal_thresholds': optimal_thresholds.tolist(),
        'elapsed_hours': elapsed_time,
        'models_used': [
            {
                'name': m['name'],
                'role': m['role'],
                'weight': float(w)
            }
            for m, w in zip(all_models, optimal_weights)
        ],
        'target_achieved': {
            'accuracy_90': final_hamming_acc >= 0.90,
            'accuracy_95': final_hamming_acc >= 0.95,
            'f1_macro_90': final_f1_macro >= 0.90,
            'f1_macro_92': final_f1_macro >= 0.92
        }
    }

    os.makedirs("./results", exist_ok=True)

    with open("./results/final_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    logger.info("\nâœ… í•™ìŠµ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: ./results/final_results.json")

    return results


if __name__ == "__main__":
    main()
