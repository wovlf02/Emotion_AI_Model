"""
ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ ë° ì•™ìƒë¸” ì‹œìŠ¤í…œ
KcELECTRA, SoongsilBERT, KLUE-RoBERTa-Large + LoRA í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”
"""

import torch
import torch.nn as nn
from transformers import (
    AutoModel, 
    AutoConfig,
    BitsAndBytesConfig
)
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training
)
from typing import Dict, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiLabelClassifier(nn.Module):
    """ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê¸°ë³¸ ë¶„ë¥˜ê¸°"""
    
    def __init__(
        self,
        model_name: str,
        num_labels: int,
        dropout_rate: float = 0.3,  # 0.1 â†’ 0.3ìœ¼ë¡œ ëŒ€í­ ì¦ê°€
        use_qlora: bool = False
    ):
        """
        Args:
            model_name: Hugging Face ëª¨ë¸ ì´ë¦„
            num_labels: ë ˆì´ë¸” ê°œìˆ˜
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
            use_qlora: QLoRA ì‚¬ìš© ì—¬ë¶€ (Large ëª¨ë¸ìš©)
        """
        super(MultiLabelClassifier, self).__init__()
        
        self.model_name = model_name
        self.num_labels = num_labels
        self.use_qlora = use_qlora
        
        # QLoRA ì„¤ì • (Large ëª¨ë¸ìš©)
        if use_qlora:
            logger.info(f"ğŸ”§ QLoRA ëª¨ë“œë¡œ {model_name} ë¡œë”© ì¤‘...")
            
            # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",  # NormalFloat 4-bit
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True  # ì¤‘ì²© ì–‘ìí™”
            )
            
            # ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸)
            self.encoder = AutoModel.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # QLoRAìš© ì¤€ë¹„
            self.encoder = prepare_model_for_kbit_training(self.encoder)
            
            # LoRA ì„¤ì •
            lora_config = LoraConfig(
                r=16,  # Rank (ë¬¸ì„œ ì „ëµëŒ€ë¡œ)
                lora_alpha=32,  # Alpha (ë¬¸ì„œ ì „ëµëŒ€ë¡œ)
                target_modules=["query", "value"],  # Q, V í–‰ë ¬ë§Œ
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.FEATURE_EXTRACTION
            )
            
            # LoRA ì–´ëŒ‘í„° ì£¼ì…
            self.encoder = get_peft_model(self.encoder, lora_config)
            self.encoder.print_trainable_parameters()
            
            hidden_size = self.encoder.config.hidden_size
        
        else:
            # ì¼ë°˜ ëª¨ë“œ (Base ëª¨ë¸ìš©)
            logger.info(f"ğŸ“¦ ì¼ë°˜ ëª¨ë“œë¡œ {model_name} ë¡œë”© ì¤‘...")
            self.encoder = AutoModel.from_pretrained(
                model_name,
                use_safetensors=True  # safetensors ìš°ì„  ì‚¬ìš©
            )
            hidden_size = self.encoder.config.hidden_size
        
        # ë¶„ë¥˜ í—¤ë“œ - Dropout 2ê°œ ë ˆì´ì–´ë¡œ ê°•í™”
        self.dropout1 = nn.Dropout(dropout_rate)
        self.dropout2 = nn.Dropout(dropout_rate * 0.5)  # ì¶”ê°€ ë“œë¡­ì•„ì›ƒ
        self.classifier = nn.Linear(hidden_size, num_labels)
        
        # ì´ˆê¸°í™”
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.zeros_(self.classifier.bias)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            input_ids: í† í° ID (batch_size, seq_len)
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ (batch_size, seq_len)
            labels: ë ˆì´ë¸” (batch_size, num_labels)
        
        Returns:
            dict: logits, loss (if labels provided)
        """
        # ì¸ì½”ë” í†µê³¼
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        # ë¶„ë¥˜
        pooled_output = self.dropout1(pooled_output)
        pooled_output = self.dropout2(pooled_output)
        logits = self.classifier(pooled_output)
        
        output_dict = {'logits': logits}
        
        return output_dict
    
    def freeze_encoder(self, num_layers_to_unfreeze: int = 2):
        """ì¸ì½”ë” ë™ê²° (ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ)"""
        if self.use_qlora:
            logger.info("QLoRA ëª¨ë“œì—ì„œëŠ” ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ê°€ ìµœì í™”ë©ë‹ˆë‹¤.")
            return
        
        # ì „ì²´ ë™ê²°
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•´ì œ
        if hasattr(self.encoder, 'encoder'):
            layers = self.encoder.encoder.layer
        elif hasattr(self.encoder, 'layer'):
            layers = self.encoder.layer
        else:
            logger.warning("ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        for layer in layers[-num_layers_to_unfreeze:]:
            for param in layer.parameters():
                param.requires_grad = True
        
        logger.info(f"ë§ˆì§€ë§‰ {num_layers_to_unfreeze}ê°œ ë ˆì´ì–´ í•™ìŠµ í™œì„±í™”")


class HybridEnsemble:
    """3-ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        num_labels: int,
        device: str = 'cuda'
    ):
        """
        Args:
            num_labels: ë ˆì´ë¸” ê°œìˆ˜
            device: ë””ë°”ì´ìŠ¤
        """
        self.num_labels = num_labels
        self.device = device
        self.models = {}
        self.weights = None
        
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ 3-ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info("="*80)
    
    def add_model(self, name: str, model: nn.Module, weight: float = 1.0):
        """ì•™ìƒë¸”ì— ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = {
            'model': model.to(self.device),
            'weight': weight
        }
        logger.info(f"âœ“ {name} ì¶”ê°€ (ê°€ì¤‘ì¹˜: {weight})")
    
    def load_models(self):
        """15ë²ˆ ë¬¸ì„œ ì „ëµëŒ€ë¡œ 3ê°œ ëª¨ë¸ ë¡œë“œ"""
        
        # ëª¨ë¸ 1: KcELECTRA-Base (ìŠ¬ë­/ìš•ì„¤ ì „ë¬¸ê°€)
        logger.info("\n[1/3] KcELECTRA-Base ë¡œë”©...")
        kcelectra = MultiLabelClassifier(
            model_name="beomi/KcELECTRA-base",
            num_labels=self.num_labels,
            dropout_rate=0.1,
            use_qlora=False
        )
        self.add_model("kcelectra", kcelectra, weight=1.0)
        
        # ëª¨ë¸ 2: SoongsilBERT-Base (ì•ˆì •ì  ë² ì´ìŠ¤ë¼ì¸)
        logger.info("\n[2/3] SoongsilBERT-Base ë¡œë”©...")
        soongsil = MultiLabelClassifier(
            model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
            num_labels=self.num_labels,
            dropout_rate=0.1,
            use_qlora=False
        )
        self.add_model("soongsil", soongsil, weight=1.0)
        
        # ëª¨ë¸ 3: KLUE-RoBERTa-Large + LoRA (ê³ ë§¥ë½ ì˜ë¯¸ë¡  ì „ë¬¸ê°€)
        logger.info("\n[3/3] KLUE-RoBERTa-Large + LoRA ë¡œë”©...")
        try:
            roberta_large = MultiLabelClassifier(
                model_name="klue/roberta-large",
                num_labels=self.num_labels,
                dropout_rate=0.1,
                use_qlora=True  # QLoRA í™œì„±í™”
            )
            self.add_model("roberta_large", roberta_large, weight=1.2)
        except Exception as e:
            logger.warning(f"âš ï¸ RoBERTa-Large ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("ğŸ’¡ Base ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            roberta_base = MultiLabelClassifier(
                model_name="klue/roberta-base",
                num_labels=self.num_labels,
                dropout_rate=0.1,
                use_qlora=False
            )
            self.add_model("roberta_base", roberta_base, weight=1.0)
        
        logger.info("\nâœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ!")
        logger.info("="*80 + "\n")
    
    def predict(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ ì†Œí”„íŠ¸ ë³´íŒ…)"""
        predictions = []
        total_weight = sum(m['weight'] for m in self.models.values())
        
        for name, model_dict in self.models.items():
            model = model_dict['model']
            weight = model_dict['weight']
            
            model.eval()
            with torch.no_grad():
                outputs = model(input_ids, attention_mask)
                logits = outputs['logits']
                probs = torch.sigmoid(logits)
                predictions.append(probs * (weight / total_weight))
        
        # ê°€ì¤‘ í‰ê· 
        ensemble_probs = torch.stack(predictions).sum(dim=0)
        
        return ensemble_probs
    
    def save_models(self, save_dir: str):
        """ëª¨ë“  ëª¨ë¸ ì €ì¥"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        for name, model_dict in self.models.items():
            model = model_dict['model']
            save_path = os.path.join(save_dir, f"{name}.pt")
            torch.save(model.state_dict(), save_path)
            logger.info(f"âœ“ {name} ì €ì¥: {save_path}")
    
    def load_model_weights(self, load_dir: str):
        """ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        import os
        
        for name, model_dict in self.models.items():
            model = model_dict['model']
            load_path = os.path.join(load_dir, f"{name}.pt")
            
            if os.path.exists(load_path):
                model.load_state_dict(torch.load(load_path, map_location=self.device))
                logger.info(f"âœ“ {name} ë¡œë“œ: {load_path}")
            else:
                logger.warning(f"âš ï¸ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {load_path}")


def create_model(model_name: str, num_labels: int, use_qlora: bool = False, dropout_rate: float = 0.3) -> nn.Module:
    """ë‹¨ì¼ ëª¨ë¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    return MultiLabelClassifier(
        model_name=model_name,
        num_labels=num_labels,
        dropout_rate=dropout_rate,
        use_qlora=use_qlora
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("="*80)
    print("ëª¨ë¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model = create_model("beomi/KcELECTRA-base", num_labels=9, use_qlora=False)
    print(f"\nâœ“ ëª¨ë¸ ìƒì„± ì„±ê³µ: {model.model_name}")
    print(f"  ë ˆì´ë¸” ìˆ˜: {model.num_labels}")
    
    # ì•™ìƒë¸” í…ŒìŠ¤íŠ¸
    # ensemble = HybridEnsemble(num_labels=9, device='cuda' if torch.cuda.is_available() else 'cpu')
    # ensemble.load_models()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
