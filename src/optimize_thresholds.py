"""
í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰
ê²€ì¦ ë°ì´í„°ë¡œ F1-Macroë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì„ê³„ê°’ ì°¾ê¸°
"""

import os
import torch
import numpy as np
import pandas as pd
import json
import logging
from tqdm import tqdm
from transformers import AutoTokenizer
from sklearn.metrics import f1_score
from scipy.optimize import minimize

from data_loader import UnsmileDataLoader
from dataset import create_dataloaders
from model import create_model

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model_and_predict_val(model_config: dict, val_df: pd.DataFrame, label_columns: list, device: str):
    """ë‹¨ì¼ ëª¨ë¸ë¡œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡"""

    model_name = model_config['name']
    model_path = f"./models/{model_name}.pt"

    if not os.path.exists(model_path):
        logger.warning(f"âš ï¸ {model_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    logger.info(f"ğŸ“¦ {model_name} ë¡œë”© ì¤‘...")

    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(model_config['hf_name'])

    # ë°ì´í„°ë¡œë”
    _, val_loader, _ = create_dataloaders(
        train_df=val_df[:1],  # dummy
        val_df=val_df,
        test_df=val_df[:1],   # dummy
        tokenizer=tokenizer,
        label_columns=label_columns,
        batch_size=16,
        max_length=128
    )

    # ëª¨ë¸ ìƒì„±
    model = create_model(
        model_name=model_config['hf_name'],
        num_labels=9,
        use_qlora=model_config.get('use_qlora', False)
    )

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(torch.load(model_path, map_location=device))
    model = model.to(device)
    model.eval()

    # ì˜ˆì¸¡
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f"Predicting {model_name}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            logits = outputs['logits']
            probs = torch.sigmoid(logits)

            all_probs.append(probs.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_probs = np.concatenate(all_probs, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    return all_probs, all_labels


def optimize_thresholds_per_class(ensemble_probs: np.ndarray, labels: np.ndarray, label_columns: list):
    """í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰ (Grid Search)"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ” í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰")
    logger.info("="*80)

    n_classes = ensemble_probs.shape[1]
    optimal_thresholds = np.zeros(n_classes)

    # í´ë˜ìŠ¤ë³„ë¡œ ìµœì  ì„ê³„ê°’ íƒìƒ‰
    for i in range(n_classes):
        best_f1 = 0
        best_thresh = 0.5

        # 0.1ë¶€í„° 0.9ê¹Œì§€ 0.01 ê°„ê²©ìœ¼ë¡œ íƒìƒ‰
        for thresh in np.arange(0.1, 0.91, 0.01):
            preds = (ensemble_probs[:, i] >= thresh).astype(int)
            f1 = f1_score(labels[:, i], preds, zero_division=0)

            if f1 > best_f1:
                best_f1 = f1
                best_thresh = thresh

        optimal_thresholds[i] = best_thresh
        logger.info(f"  {label_columns[i]:20s}: {best_thresh:.2f} (F1: {best_f1:.4f})")

    logger.info("="*80 + "\n")

    return optimal_thresholds


def optimize_thresholds_global(ensemble_probs: np.ndarray, labels: np.ndarray):
    """ì „ì—­ ìµœì  ì„ê³„ê°’ íƒìƒ‰ (F1-Macro ìµœëŒ€í™”)"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ì „ì—­ ìµœì  ì„ê³„ê°’ íƒìƒ‰ (F1-Macro ìµœëŒ€í™”)")
    logger.info("="*80)

    def objective(thresholds):
        """F1-Macroë¥¼ ìµœëŒ€í™” (ìŒìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìµœì†Œí™” ë¬¸ì œë¡œ)"""
        thresholds = np.clip(thresholds, 0.05, 0.95)

        preds = np.zeros_like(ensemble_probs, dtype=int)
        for i in range(len(thresholds)):
            preds[:, i] = (ensemble_probs[:, i] >= thresholds[i]).astype(int)

        f1 = f1_score(labels, preds, average='macro', zero_division=0)
        return -f1  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜

    # ì´ˆê¸°ê°’: 0.5
    initial_thresholds = np.ones(ensemble_probs.shape[1]) * 0.5

    # Nelder-Mead ìµœì í™”
    result = minimize(
        objective,
        initial_thresholds,
        method='Nelder-Mead',
        options={'maxiter': 1000, 'disp': True}
    )

    optimal_thresholds = np.clip(result.x, 0.05, 0.95)
    best_f1 = -result.fun

    logger.info(f"\nâœ… ìµœì í™” ì™„ë£Œ!")
    logger.info(f"  ìµœì  F1-Macro: {best_f1:.4f}")
    logger.info(f"  ìµœì  ì„ê³„ê°’: {optimal_thresholds}")
    logger.info("="*80 + "\n")

    return optimal_thresholds


def main():
    """ë©”ì¸ ì„ê³„ê°’ ìµœì í™” íŒŒì´í”„ë¼ì¸"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰")
    logger.info("="*80)

    # ë””ë°”ì´ìŠ¤
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"\nğŸ’» Device: {device}")

    # 1. ë°ì´í„° ë¡œë“œ
    logger.info("\nğŸ“Š Step 1: ë°ì´í„° ë¡œë”©")
    data_loader = UnsmileDataLoader(data_dir="./data")
    train_df, val_df, test_df = data_loader.load_processed_data()
    label_columns = data_loader.label_columns

    logger.info(f"âœ“ Val ìƒ˜í”Œ ìˆ˜: {len(val_df)}")

    # 2. ëª¨ë¸ ì„¤ì • (3ê°œ ëª¨ë¸)
    models_config = [
        {
            'name': 'kcelectra',
            'hf_name': 'beomi/KcELECTRA-base',
            'use_qlora': False
        },
        {
            'name': 'soongsil',
            'hf_name': 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',
            'use_qlora': False
        },
        {
            'name': 'roberta_base',
            'hf_name': 'klue/roberta-base',
            'use_qlora': False
        }
    ]

    # 3. ê° ëª¨ë¸ë¡œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
    logger.info("\nğŸ“Š Step 2: ê° ëª¨ë¸ë¡œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡")
    all_model_probs = []
    labels = None

    for config in models_config:
        result = load_model_and_predict_val(config, val_df, label_columns, device)

        if result is not None:
            probs, lbls = result
            all_model_probs.append(probs)
            if labels is None:
                labels = lbls

    if len(all_model_probs) == 0:
        raise ValueError("ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")

    # 4. ì•™ìƒë¸” (í‰ê· )
    logger.info("\nğŸ“Š Step 3: ì•™ìƒë¸” (ë‹¨ìˆœ í‰ê· )")
    ensemble_probs = np.mean(all_model_probs, axis=0)
    logger.info(f"âœ“ ì•™ìƒë¸” ì™„ë£Œ: {len(all_model_probs)}ê°œ ëª¨ë¸")

    # 5. í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰
    logger.info("\nğŸ“Š Step 4: í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ íƒìƒ‰")
    optimal_thresholds_class = optimize_thresholds_per_class(ensemble_probs, labels, label_columns)

    # 6. ì „ì—­ ìµœì í™” (F1-Macro ìµœëŒ€í™”)
    logger.info("\nğŸ“Š Step 5: ì „ì—­ ìµœì í™” (F1-Macro ìµœëŒ€í™”)")
    optimal_thresholds_global = optimize_thresholds_global(ensemble_probs, labels)

    # 7. ë‘ ë°©ë²• ë¹„êµ
    logger.info("\nğŸ“Š Step 6: ë‘ ë°©ë²• ë¹„êµ")

    # í´ë˜ìŠ¤ë³„ ìµœì í™” ê²°ê³¼
    preds_class = np.zeros_like(ensemble_probs, dtype=int)
    for i in range(len(optimal_thresholds_class)):
        preds_class[:, i] = (ensemble_probs[:, i] >= optimal_thresholds_class[i]).astype(int)

    f1_class = f1_score(labels, preds_class, average='macro', zero_division=0)

    # ì „ì—­ ìµœì í™” ê²°ê³¼
    preds_global = np.zeros_like(ensemble_probs, dtype=int)
    for i in range(len(optimal_thresholds_global)):
        preds_global[:, i] = (ensemble_probs[:, i] >= optimal_thresholds_global[i]).astype(int)

    f1_global = f1_score(labels, preds_global, average='macro', zero_division=0)

    logger.info(f"\ní´ë˜ìŠ¤ë³„ ìµœì í™”: F1-Macro {f1_class:.4f}")
    logger.info(f"ì „ì—­ ìµœì í™”:     F1-Macro {f1_global:.4f}")

    # ë” ì¢‹ì€ ë°©ë²• ì„ íƒ
    if f1_global >= f1_class:
        logger.info(f"\nâœ… ì „ì—­ ìµœì í™” ë°©ë²• ì„ íƒ (F1: {f1_global:.4f})")
        final_thresholds = optimal_thresholds_global
        final_f1 = f1_global
    else:
        logger.info(f"\nâœ… í´ë˜ìŠ¤ë³„ ìµœì í™” ë°©ë²• ì„ íƒ (F1: {f1_class:.4f})")
        final_thresholds = optimal_thresholds_class
        final_f1 = f1_class

    # 8. ìµœì¢… ì„ê³„ê°’ ì¶œë ¥
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ ìµœì¢… ìµœì  ì„ê³„ê°’")
    logger.info("="*80)

    for i, (name, thresh) in enumerate(zip(label_columns, final_thresholds)):
        logger.info(f"  {name:20s}: {thresh:.4f}")

    logger.info(f"\nâœ… ìµœì¢… F1-Macro: {final_f1:.4f}")
    logger.info("="*80 + "\n")

    # 9. ê²°ê³¼ ì €ì¥
    result_dict = {
        'optimal_thresholds': final_thresholds.tolist(),
        'threshold_dict': {name: float(thresh) for name, thresh in zip(label_columns, final_thresholds)},
        'val_f1_macro': float(final_f1),
        'method': 'global_optimization' if f1_global >= f1_class else 'class_wise_optimization'
    }

    output_path = "./results/optimal_thresholds.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result_dict, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… ìµœì  ì„ê³„ê°’ ì €ì¥: {output_path}")

    logger.info("\n" + "="*80)
    logger.info("âœ… ì„ê³„ê°’ ìµœì í™” ì™„ë£Œ!")
    logger.info("="*80)


if __name__ == "__main__":
    main()

